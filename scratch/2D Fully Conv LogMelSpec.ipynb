{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from fastai.conv_learner import ConvLearner\n",
    "from fastai.core import to_np\n",
    "from fastai.dataloader import DataLoader\n",
    "from fastai.dataset import get_cv_idxs, split_by_idx, ArraysIndexDataset, ModelData\n",
    "from fastai.metrics import accuracy, accuracy_np\n",
    "from fastai.model import fit, predict\n",
    "from fastai.text import SortishSampler\n",
    "\n",
    "from data_loading_utils import load_audio_files, read_file\n",
    "from preprocessing_utils import load_features\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data/')\n",
    "TRAIN_PATH = PATH/'audio_train'\n",
    "TEST_PATH = PATH/'audio_test'\n",
    "\n",
    "sample_rate = 44100\n",
    "n_features = 60\n",
    "n_fft = 1024\n",
    "hop_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(PATH/'train.csv')\n",
    "\n",
    "labels = sorted(train.label.unique())\n",
    "label_idx = {label:i for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data..\n",
      "Loaded features for 9473 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9473, 9473)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = load_features(TRAIN_PATH,\n",
    "                  filenames=train.fname, \n",
    "                  feature_name='log_mel_spec',\n",
    "                  n_fft=n_fft, \n",
    "                  hop_length=hop_length,\n",
    "                  n_features=n_features)\n",
    "y = train.label.apply(lambda l: label_idx[l]).values\n",
    "len(x), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data..\n",
      "Loaded features for 9400 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9400, 9400)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(PATH/'sample_submission.csv')\n",
    "test_x = load_features(TEST_PATH, \n",
    "                       filenames=test.fname, \n",
    "                       feature_name='log_mel_spec', \n",
    "                       n_fft=n_fft, \n",
    "                       hop_length=hop_length,\n",
    "                       n_features=n_features)\n",
    "test_y = np.zeros(len(test_x))\n",
    "len(test_x), len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from blake\n",
    "def get_trn_val_split(x, y, val_pct=0.15):\n",
    "    val_idxs = get_cv_idxs(len(x), val_pct=val_pct)\n",
    "    if isinstance(x, list):\n",
    "        return [([arr[i] for i in val_idxs], [arr[i] for i in range(len(arr)) if i not in val_idxs]) for arr in [x,y]]\n",
    "    else:\n",
    "        return split_by_idx(val_idxs, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8053, 8053, 1420, 1420)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((val_x, trn_x), (val_y, trn_y)) = get_trn_val_split(x, y, 0.15)\n",
    "len(trn_x), len(trn_y), len(val_x), len(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDatasetDataset(ArraysIndexDataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        super().__init__(x, y, transform)\n",
    "    def get_c(self): \n",
    "        return max(self.y) + 1\n",
    "    def get_sz(self):\n",
    "        return self.x[0].shape[0]\n",
    "    def get_x(self, i):\n",
    "        return self.x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataLoader(DataLoader):\n",
    "    def get_batch(self, indexes):\n",
    "        batch_data = [self.dataset[i] for i in indexes]\n",
    "        x_lens = [item[0].shape[1] for item in batch_data]\n",
    "        if len(np.unique(x_lens)) > 1:\n",
    "            max_len = np.max(x_lens)\n",
    "            for i, item in enumerate(batch_data):\n",
    "                x, y = item\n",
    "                clip_len = x.shape[1]\n",
    "                pad_mode = 'wrap' if clip_len > 1 else 'constant'\n",
    "                x = np.pad(x, ((0, 0), (0, max_len-clip_len)), pad_mode)\n",
    "                batch_data[i] = x, y\n",
    "        return self.np_collate(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "\n",
    "def get_dl(x, y, bs):\n",
    "    ds = AudioDatasetDataset(x, y)\n",
    "    sampler = SortishSampler(ds, key=lambda x: ds[x][0].shape[1], bs=bs)\n",
    "    dl = AudioDataLoader(ds, bs, sampler)\n",
    "    return dl\n",
    "\n",
    "trn_dl = get_dl(trn_x, trn_y, bs)\n",
    "val_dl = get_dl(val_x, val_y, bs)\n",
    "test_dl = get_dl(test_x, test_y, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 60, 2197]), torch.Size([32]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, y1 = next(iter(trn_dl))\n",
    "x1.shape, y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(n_in, n_out, kernel_size=3, max_pool=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(n_in, n_out, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(n_out, n_out, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(max_pool),\n",
    "        nn.Dropout2d(0.1)\n",
    "    )\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super().__init__()\n",
    "        self.lambd = lambd\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        self.debug = False\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            Lambda(lambda x: x.view(x.shape[0], 1, x.shape[1], x.shape[2])),\n",
    "            conv_block(1, 16, 9, 2),\n",
    "            conv_block(16, 32, 3, 2),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.05),\n",
    "            nn.Conv2d(64, n_classes, 3),\n",
    "            Lambda(lambda x: x.view(x.shape[0], n_classes, -1)),\n",
    "            Lambda(lambda x: torch.mean(x, dim=2))\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapk_np(preds, targs, k=3):\n",
    "    preds = np.argsort(-preds, axis=1)[:, :k]\n",
    "    score = 0.0\n",
    "    for i in range(k):\n",
    "        num_hits = (preds[:, i] == targs).sum()\n",
    "        score += num_hits * (1.0 / (i+1.0))\n",
    "    score /= preds.shape[0]\n",
    "    return score\n",
    "\n",
    "def mapk(preds, targs, k=3):\n",
    "    return mapk_np(to_np(preds), to_np(targs), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ModelData(PATH, trn_dl, val_dl, test_dl)\n",
    "model = AudioCNN(len(labels)).cuda()\n",
    "opt = optim.Adam\n",
    "metrics = [accuracy, mapk]\n",
    "loss = F.cross_entropy\n",
    "learn = ConvLearner.from_model_data(model, md, crit=loss, metrics=metrics, opt_fn=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()\n",
    "# learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 1e-2\n",
    "# learn.fit(lr, 1, wds=[1e-7], cycle_len=50, use_clr_beta=(10, 25, 0.95, 0.85))\n",
    "\n",
    "# learn.save_cycle('2d_full_conv_clr_v2', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load_cycle('2d_full_conv_clr_v2', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.691, Val MAP: 0.766\n"
     ]
    }
   ],
   "source": [
    "learn.model.eval()\n",
    "val_preds = learn.predict_with_targs()\n",
    "\n",
    "val_acc = accuracy_np(*val_preds)\n",
    "val_map = mapk_np(*val_preds)\n",
    "\n",
    "print(f'Val Acc: {val_acc:.3f}, Val MAP: {val_map:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
