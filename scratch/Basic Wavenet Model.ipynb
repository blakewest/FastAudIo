{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/blakewest/anaconda/envs/fastaudio/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import IPython\n",
    "import IPython.display as ipd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "from scipy.io import wavfile\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from fastai import *\n",
    "from fastai.dataset import *\n",
    "from fastai.learner import *\n",
    "from fastai.text import *\n",
    "from shutil import copyfile\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "# torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('../../fastai/courses/dl2/data/freesound/')\n",
    "TRAIN_PATH = DATA_PATH/'audio_train_sample'\n",
    "TEST_PATH = DATA_PATH/'audio_test'\n",
    "LABEL_PATH = DATA_PATH/'train_sample.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(LABEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicAudioModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 9),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, 9),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(16),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv1d(16, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(8),\n",
    "            nn.Dropout(0.01),\n",
    "            nn.Conv1d(32, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 256, 3),\n",
    "            nn.Dropout(0.01),\n",
    "            nn.MaxPool1d(4),\n",
    "            nn.Conv1d(256, num_classes, 3),\n",
    "            Lambda(lambda tensor: torch.mean(tensor, 2, keepdim=True)),\n",
    "            Lambda(lambda tensor: tensor.view(tensor.shape[0], -1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "    \n",
    "class ResLayer(nn.Module):\n",
    "    def __init__(self, num_filters, dilation, kernel_size=3):\n",
    "        super().__init__()\n",
    "        # This should return an output of equal size.\n",
    "        self.conv_model = nn.Sequential(\n",
    "            nn.Conv1d(num_filters, num_filters, kernel_size=kernel_size, dilation=dilation, bias=False, padding=kernel_size // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(num_filters, momentum=0.01),\n",
    "            nn.Conv1d(num_filters, num_filters, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        convolved = self.conv_model(x)\n",
    "        # Dilation lop off like (n=dilation) samples from the end of the input, which means you can't\n",
    "        # add the res block. So the padding fixes that, and lets us do the residual addition.\n",
    "        padding = nn.ReplicationPad1d((0, x.size()[-1] - convolved.size()[-1]))\n",
    "        return x.add(padding(convolved))\n",
    "    \n",
    "class AudioResBlock(nn.Module):\n",
    "    def __init__(self, num_filters, num_layers):\n",
    "        super().__init__()\n",
    "        dilation = 1\n",
    "        layers = []\n",
    "        for layer in range(num_layers):\n",
    "            cur_dilation = dilation * (2 ** layer)\n",
    "            layers += [ResLayer(num_filters, cur_dilation)]\n",
    "            \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class WavenetEncoder(nn.Module):\n",
    "    def __init__(self, num_classes, num_filters=64, num_layers_per_res_block=1, bottleneck_filters=64, bs=16):\n",
    "        super().__init__()\n",
    "        self.num_filters = num_filters\n",
    "        self.init_conv = nn.Conv1d(1, num_filters, 1, bias=False)\n",
    "        self.temporal_encoder = self.get_temporal_encoder(num_filters, num_layers_per_res_block)\n",
    "        self.bottleneck = nn.Conv1d(num_filters, bottleneck_filters, 1)\n",
    "        self.classifier = self.get_classifier(bottleneck_filters, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.encode(x))\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.init_conv(x)\n",
    "        x = self.temporal_encoder(x)\n",
    "        return self.bottleneck(x)\n",
    "    \n",
    "    def get_temporal_encoder(self, num_filters, num_layers):\n",
    "        return AudioResBlock(num_filters, num_layers)\n",
    "    \n",
    "    def get_classifier(self, num_ch_in, num_classes):\n",
    "        return nn.Sequential(\n",
    "            # This is averagies each channel to a resolution 512 samples, or ~32ms. Taken from https://arxiv.org/pdf/1704.01279.pdf\n",
    "            nn.AvgPool1d(512),\n",
    "            # Create a convolution filter for each possible class\n",
    "            nn.Conv1d(num_ch_in, num_classes, 1),\n",
    "            # Averaging each classes across time, so you get 1 score for the whole sample for each classes\n",
    "            Lambda(lambda tensor: torch.mean(tensor, 2, keepdim=True)),\n",
    "            # Turn into a 1D tensor with a score for each class. PyTorch will auto softmax this if using Cross Ent Loss\n",
    "            Lambda(lambda tensor: tensor.view(tensor.shape[0], -1)),\n",
    "        )\n",
    "        \n",
    "        \n",
    "# def _encode(self,sample):\n",
    "#     sample = self.en_causal_layer(sample)\n",
    "\n",
    "#     for i,(dilation_layer,dense_layer) in enumerate(zip(self.en_dilation_layer_stack,self.en_dense_layer_stack)):\n",
    "\n",
    "#         current = sample\n",
    "\n",
    "#         sample = F.relu(sample)\n",
    "#         sample = dilation_layer(sample)\n",
    "#         sample = F.relu(sample)\n",
    "#         dense_layer is 1x1 conv\n",
    "#         sample = dense_layer(sample)\n",
    "#         _,_,current_length = sample.size()\n",
    "#         current_in_sliced = current[:,:,-current_length:]\n",
    "#         sample = sample + current_in_sliced\n",
    "\n",
    "#     sample = self.bottleneck_layer(sample)  \n",
    "#    # print(sample.size())  \n",
    "#     pool1d = nn.AvgPool1d(self.en_pool_kernel_size)\n",
    "#     sample = pool1d(sample)\n",
    "#     return sample\n",
    "\n",
    "def get_trn_val_split(x, y, val_pct=0.15):\n",
    "    val_idxs = get_cv_idxs(len(x), val_pct=val_pct)\n",
    "    if isinstance(x, list):\n",
    "        return [([arr[i] for i in val_idxs], [arr[i] for i in range(len(arr)) if i not in val_idxs]) for arr in [x,y]]\n",
    "    else:\n",
    "        return split_by_idx(val_idxs, x, y)\n",
    "\n",
    "class AudioLearner(Learner):\n",
    "    def __init__(self, data, models, **kwargs):\n",
    "        super().__init__(data, models, **kwargs)\n",
    "\n",
    "    def _get_crit(self, data):\n",
    "        return F.cross_entropy\n",
    "\n",
    "def load_audio_from_df(trn_path, trn_df, sample_rate=16000):\n",
    "    return [retrieve_file(str(trn_path) + '/' + trn_df['fname'][i], sample_rate=sample_rate) for i in range(len(trn_df))]\n",
    "\n",
    "def retrieve_file(filepath, sample_rate=16000):\n",
    "    data, _ = librosa.core.load(filepath, sr=sample_rate, res_type='kaiser_fast')\n",
    "    return data\n",
    "\n",
    "def preprocess_audio(audio_files):\n",
    "    norm = librosa.util.normalize\n",
    "    trimmed_xs = [librosa.effects.trim(norm(x))[0] for x in audio_files]\n",
    "    return [x.reshape(1, x.shape[0]) for x in trimmed_xs]\n",
    "\n",
    "def preprocess_ys(labels, one_hot=False):\n",
    "    if isinstance(labels[0], str):\n",
    "        tok2int = {v:k for k,v in enumerate(np.unique(labels))}\n",
    "        labels = np.array([tok2int[tok] for tok in labels])\n",
    "    num_classes = len(np.unique(labels))\n",
    "    if one_hot:\n",
    "        return [one_hot(labels[i], num_classes).reshape(1, num_classes) for i in range(len(labels))]\n",
    "    else:\n",
    "        return labels\n",
    "\n",
    "class AudioModelData():\n",
    "    def __init__(self, path, trn_ds, val_ds, test_ds=None, bs=64, sample_rate=16000):\n",
    "        self.path = path\n",
    "        self.bs = bs\n",
    "        self.trn_ds, self.val_ds, self.test_ds = trn_ds, val_ds, test_ds\n",
    "        self.trn_dl = AudioDataLoader(trn_ds, bs, sampler=SortishSampler(trn_ds, key=lambda x: len(trn_ds[x][0][0]), bs=bs))\n",
    "        self.val_dl = AudioDataLoader(val_ds, bs, sampler=SortishSampler(val_ds, key=lambda x: len(val_ds[x][0][0]), bs=bs))\n",
    "        self.test_dl = AudioDataLoader(test_ds, bs, sampler=SortishSampler(test_ds, key=lambda x: len(test_ds[x][0][0]),bs=bs)) if test_ds is not None else None\n",
    "        self.num_classes = self.trn_dl.dataset.get_c()\n",
    "        self.sz = self.trn_ds.get_x(1).size\n",
    "\n",
    "    @classmethod\n",
    "    def from_path_and_dataframes(cls, trn_path, trn_df, test_path=None, test_df=None, val_path=None, val_df=None, val_pct=0.15, model_path='./', bs=64):\n",
    "        xs = load_audio_from_df(trn_path, trn_df)\n",
    "        xs = preprocess_audio(x)\n",
    "        ys = preprocess_ys(trn_df['label'])\n",
    "        if test_path is not None:\n",
    "            text_xs = load_audio_from_df(test_path, test_df)\n",
    "            text_xs = preprocess_audio(test_xs)\n",
    "        else:\n",
    "            test_x = None\n",
    "        return cls.from_array(xs, ys, test_xs, bs=bs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_array(cls, trn_x, trn_y, test_x=None, val_pct=0.15, bs=64, model_path=\"./\", **kwargs):\n",
    "        ((val_x, trn_x), (val_y, trn_y)) = get_trn_val_split(trn_x, trn_y, val_pct)\n",
    "        trn_ds = AudioDataset(trn_x, trn_y)\n",
    "        val_ds = AudioDataset(val_x, val_y)\n",
    "        test_ds = AudioDataset(test_x, test_y) if test_x is not None else None\n",
    "        return cls(model_path, trn_ds, val_ds, test_ds, bs=bs)\n",
    "\n",
    "    def get_model(self, model_params={\"type\": \"basic\"}, optimizer=torch.optim.Adam):\n",
    "        if model_params['type'] == \"basic\":\n",
    "            model = BasicAudioModel(self.num_classes)\n",
    "        elif model_params['type'] == \"wavenet\":\n",
    "            model = WavenetEncoder(self.num_classes)\n",
    "        else:\n",
    "            raise \"Unknown model type: \" + model_params[\"type\"]\n",
    "        model = SingleModel(to_gpu(model))\n",
    "        return AudioLearner(self, model, opt_fn=optimizer)\n",
    "\n",
    "class AudioDataLoader(DataLoader):\n",
    "    def get_batch(self, indexes):\n",
    "        batch_data = [self.dataset[i] for i in indexes]\n",
    "        x_lens = [len(item[0][0]) for item in batch_data]\n",
    "        if len(np.unique(x_lens)) > 1:\n",
    "            max_len = np.max(x_lens)\n",
    "            for i, item in enumerate(batch_data):\n",
    "                item = list(item)\n",
    "                clip_len = len(item[0][0])\n",
    "                item[0] = np.pad(item[0], ((0,0), (0, max_len-clip_len)), 'wrap')\n",
    "                batch_data[i] = tuple(item)\n",
    "        return self.np_collate(batch_data)\n",
    "    \n",
    "class AudioDataLoader(DataLoader):\n",
    "    def get_batch(self, indexes):\n",
    "        batch_data = [self.dataset[i] for i in indexes]\n",
    "        x_lens = [item[0].shape[1] for item in batch_data]\n",
    "        if len(np.unique(x_lens)) > 1:\n",
    "            max_len = np.max(x_lens)\n",
    "            for i, item in enumerate(batch_data):\n",
    "                x, y = item\n",
    "                clip_len = x.shape[1]\n",
    "                pad_mode = 'wrap' if clip_len > 1 else 'constant'\n",
    "                x = np.pad(x, ((0, 0), (0, max_len-clip_len)), pad_mode)\n",
    "                batch_data[i] = x, y\n",
    "        return self.np_collate(batch_data)\n",
    "\n",
    "class AudioDataset(BaseDataset):\n",
    "    def __init__(self, xs, ys, transforms=None):\n",
    "        if isinstance(ys[0], str):\n",
    "            ys = preprocess_ys(ys)\n",
    "        self.ys = ys\n",
    "        self.xs = xs\n",
    "        assert(len(xs) == len(ys)), \"Length of xs does not equal length of ys\"\n",
    "        super().__init__(transforms)\n",
    "\n",
    "    def get_x(self, i):\n",
    "        return self.xs[i]\n",
    "\n",
    "    def get_y(self, i):\n",
    "        return self.ys[i]\n",
    "\n",
    "    def get_n(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def get_sz(self):\n",
    "        return self.get_x(1).shape[0]\n",
    "\n",
    "    def get_c(self):\n",
    "        return int(np.max(self.ys) + 1)\n",
    "\n",
    "def create_sample_data(path, percent=0.1, sample_path=None, overwrite=False, labels_df=None):\n",
    "    sample_path = path + '_sample' if sample_path is None else sample_path\n",
    "    if not os.path.exists(sample_path):\n",
    "            print(\"Creating folder for the sample set...\", sample_path)\n",
    "            os.mkdir(sample_path)\n",
    "    existing_sample_files = glob(sample_path + '/*')\n",
    "    if len(existing_sample_files) > 0:\n",
    "        if not overwrite:\n",
    "            print(\"Sample already exists. Pass overwrite=True to delete and redo\")\n",
    "            return\n",
    "        else:\n",
    "            for file in existing_sample_files:\n",
    "                os.remove(file)\n",
    "    print(\"Saving a\", percent * 100, \"percent sample to\", sample_path)\n",
    "    for filepath in glob(path + '/*'):\n",
    "        if np.random.random() < percent:\n",
    "            fname = filepath.split('/')[-1]\n",
    "            copyfile(filepath, sample_path + '/' + fname)\n",
    "    if labels_df:\n",
    "        sample_fnames = [filepath.split('/')[-1] for filepath in glob(sample_path + '/*')]\n",
    "        labels_df[labels_df['fname'].isin(sample_fnames)].to_csv(path + '../train_sample.csv')\n",
    "\n",
    "def save_data(data, filename):\n",
    "    with open(filename, 'wb') as outfile:\n",
    "        pickle.dump(data, outfile, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename, 'rb') as infile:\n",
    "        result = pickle.load(infile)\n",
    "    return result\n",
    "\n",
    "def display_sample(train, category=None):\n",
    "    sample = train[train['label'] == category].sample() if category else train.sample()\n",
    "    fname = str(TRN_PATH/sample['fname'].values[0])\n",
    "    print(sample)\n",
    "    return ipd.Audio(fname)\n",
    "\n",
    "def munge_and_save_data(trn_path, trn_df, x_filepath, y_filepath):\n",
    "    print(\"Loading files...\")\n",
    "    xs = load_audio_from_df(trn_path, trn_df)\n",
    "    print(\"Processing audio...\")\n",
    "    xs = preprocess_audio(xs)\n",
    "    print(\"Processing labels...\")\n",
    "    ys = preprocess_ys(trn_df['label'])\n",
    "    print(\"Saving xs and ys...\")\n",
    "    save_data(xs, x_filepath)\n",
    "    save_data(ys, y_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# munge_and_save_data(TRAIN_PATH, train, x_filepath=DATA_PATH/'all_audio.pkl', y_filepath=DATA_PATH/'all_labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = load_data(DATA_PATH/'all_audio.pkl')\n",
    "ys = load_data(DATA_PATH/'all_labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188416, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[0].size, ys[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = xs[2]\n",
    "test_x = V(test_x).view(1, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 212480])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_layer = ResLayer(256, 8, 8)\n",
    "init_conv = nn.Conv1d(1, 256, 1)\n",
    "test_model = nn.Sequential(init_conv, res_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256, 64960]), torch.Size([1, 1, 64960]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = init_conv(test_x)\n",
    "test = res_layer.conv_model(test)\n",
    "padding = nn.ReplicationPad1d((0, test_x.size()[-1] - test.size()[-1]))\n",
    "test = padding(test)\n",
    "test.size(), test_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = test_model(test_x).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = AudioModelData.from_array(xs[:16],ys[:16], bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = md.get_model(model_params={\"type\": \"wavenet\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9886ecf09376447b9a6c65f491f018fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-bd2e9e247272>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/fastaudio/lib/python3.6/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwarm_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/fastaudio/lib/python3.6/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mswa_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswa_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswa_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             swa_eval_freq=swa_eval_freq, **kwargs)\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/fastaudio/lib/python3.6/site-packages/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_stepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/fastaudio/lib/python3.6/site-packages/fastai/model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, xs, y, epoch)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_scale\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mupdate_fp32_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp32_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_scale\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/fastaudio/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/fastaudio/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation"
     ]
    }
   ],
   "source": [
    "learner.fit(lrs=0.001, n_cycle=1, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad54ae462b5f46d0b77188eb58038982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# learner.fit(lrs=0.001, n_cycle=1, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
