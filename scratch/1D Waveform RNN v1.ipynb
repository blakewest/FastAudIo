{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Lambda\n",
    "\n",
    "from fastai.conv_learner import ConvLearner\n",
    "from fastai.core import *\n",
    "from fastai.dataloader import DataLoader\n",
    "from fastai.dataset import get_cv_idxs, split_by_idx, ArraysIndexDataset, ModelData\n",
    "from fastai.metrics import accuracy\n",
    "from fastai.model import fit, predict\n",
    "from fastai.text import SortishSampler\n",
    "\n",
    "from data_loading_utils import load_audio_files, read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data/')\n",
    "TRAIN_PATH = PATH/'audio_train_16KHz'\n",
    "TEST_PATH = PATH/'audio_test_16KHz'\n",
    "\n",
    "sample_rate = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57aebbcd8e10449492c181cc7fa8132e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9473, 9473)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(PATH/'train.csv')\n",
    "\n",
    "labels = sorted(train.label.unique())\n",
    "label_idx = {label:i for i, label in enumerate(labels)}\n",
    "\n",
    "x = load_audio_files(TRAIN_PATH, filenames=train.fname, trimmed=True)\n",
    "y = train.label.apply(lambda l: label_idx[l]).values\n",
    "len(x), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8053, 8053, 1420, 1420)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from blake\n",
    "def get_trn_val_split(x, y, val_pct=0.15):\n",
    "    val_idxs = get_cv_idxs(len(x), val_pct=val_pct)\n",
    "    if isinstance(x, list):\n",
    "        return [([arr[i] for i in val_idxs], [arr[i] for i in range(len(arr)) if i not in val_idxs]) for arr in [x,y]]\n",
    "    else:\n",
    "        return split_by_idx(val_idxs, x, y)\n",
    "    \n",
    "((val_x, trn_x), (val_y, trn_y)) = get_trn_val_split(x, y, 0.15)\n",
    "len(trn_x), len(trn_y), len(val_x), len(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDatasetDataset(ArraysIndexDataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        super().__init__(x, y, transform)\n",
    "    def get_c(self): \n",
    "        return max(self.y) + 1\n",
    "    def get_sz(self):\n",
    "        return self.x[0].shape[0]\n",
    "    def get_x(self, i):\n",
    "        return self.x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataLoader1d(DataLoader):\n",
    "    def get_batch(self, indexes):\n",
    "        batch_data = [self.dataset[i] for i in indexes]\n",
    "        x_lens = [item[0].shape[0] for item in batch_data]\n",
    "        if len(np.unique(x_lens)) > 1:\n",
    "            max_len = np.max(x_lens)\n",
    "            for i, item in enumerate(batch_data):\n",
    "                x, y = item\n",
    "                clip_len = x.shape[0]\n",
    "                pad_mode = 'wrap' if clip_len > 1 else 'constant'\n",
    "                x = np.pad(x, (0, max_len-clip_len), pad_mode)\n",
    "                batch_data[i] = x, y\n",
    "        return self.np_collate(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "\n",
    "# ArraysIndexDataset expects np arrays\n",
    "trn_y, val_y = np.array(trn_y), np.array(val_y)\n",
    "\n",
    "trn_ds = AudioDatasetDataset(trn_x, trn_y)\n",
    "val_ds = AudioDatasetDataset(val_x, val_y)\n",
    "trn_dl = AudioDataLoader1d(trn_ds, \n",
    "                           sampler=SortishSampler(trn_ds, key=lambda x: trn_ds[x][0].shape[0], bs=bs),\n",
    "                           batch_size=bs)\n",
    "val_dl = AudioDataLoader1d(val_ds,\n",
    "                           sampler=SortishSampler(val_ds, key=lambda x: val_ds[x][0].shape[0], bs=bs),\n",
    "                           batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 22080]), torch.Size([64]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, y1 = next(iter(trn_dl))\n",
    "x1.size(), y1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super().__init__()\n",
    "        self.lambd = lambd\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class RawAudioRNN(nn.Module):\n",
    "    def __init__(self, n_hidden, n_classes, n_layers=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.n_final_conv = 64\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.n_final_conv, n_hidden, n_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(n_hidden, n_classes)\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Add in channel dimension\n",
    "            Lambda(lambda x: x.view(x.shape[0], 1, x.shape[1])),\n",
    "    \n",
    "            nn.Conv1d(1, 16, kernel_size=9, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(16, 16, kernel_size=9, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(16),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Conv1d(16, 32, kernel_size=3, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(32, 32, kernel_size=3, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(4),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(64, 64, kernel_size=3, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(4),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Conv1d(64, self.n_final_conv, kernel_size=3, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(8),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv_layers(x)\n",
    "        \n",
    "        bs, input_size, sequence_length = out.size()\n",
    "        out = out.view(bs, sequence_length, input_size)\n",
    "                \n",
    "        h0 = V(torch.zeros(self.n_layers, bs, self.n_hidden))\n",
    "        c0 = V(torch.zeros(self.n_layers, bs, self.n_hidden))\n",
    "        \n",
    "        out, _ = self.lstm(out, (h0, c0))            \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapk_np(preds, targs, k=3):\n",
    "    preds = np.argsort(-preds, axis=1)[:, :k]\n",
    "    score = 0.0\n",
    "    for i in range(k):\n",
    "        num_hits = (preds[:, i] == targs).sum()\n",
    "        score += num_hits * (1.0 / (i+1.0))\n",
    "    score /= preds.shape[0]\n",
    "    return score\n",
    "\n",
    "def mapk(preds, targs, k=3):\n",
    "    return mapk_np(to_np(preds), to_np(targs), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 41])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RawAudioRNN(128, len(labels)).cuda()\n",
    "model(x1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ModelData(PATH, trn_dl, val_dl)\n",
    "opt = optim.Adam\n",
    "metrics = [accuracy, mapk]\n",
    "loss = F.cross_entropy\n",
    "learn = ConvLearner.from_model_data(model, md, crit=loss, metrics=metrics, opt_fn=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485ded32d61e4d3eaadf9c123ba2b5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy   mapk       \n",
      "    0      3.671593   3.665341   0.037324   0.060329  \n",
      "    1      3.656525   3.651369   0.035211   0.062207  \n",
      "    2      3.59219    3.524137   0.08662    0.129108  \n",
      "    3      3.494444   3.503371   0.059155   0.101995  \n",
      "    4      3.423718   3.348474   0.11831    0.176878  \n",
      "    5      3.315465   3.242729   0.148592   0.204577  \n",
      "    6      3.165086   3.193549   0.137324   0.202347  \n",
      "    7      3.09327    3.222853   0.139437   0.201761  \n",
      "    8      3.081697   3.062034   0.167606   0.244601  \n",
      "    9      3.046067   2.936918   0.197183   0.28439   \n",
      "    10     2.939611   2.938675   0.195775   0.284155  \n",
      "    11     2.909037   2.818861   0.214085   0.307864  \n",
      "    12     2.75819    2.846054   0.20493    0.305516  \n",
      "    13     2.779248   2.926299   0.195775   0.28392   \n",
      "    14     2.736981   2.766463   0.230986   0.327347  \n",
      "    15     2.696654   2.705832   0.238732   0.33838   \n",
      "    16     2.64717    2.705368   0.244366   0.341901  \n",
      "    17     2.562515   2.62762    0.261972   0.366784  \n",
      "    18     2.531388   2.771542   0.223239   0.330399  \n",
      "    19     2.477109   2.618874   0.26338    0.366549  \n",
      "    20     2.427809   2.568505   0.276056   0.376408  \n",
      "    21     2.381637   2.425714   0.307746   0.417606  \n",
      "    22     2.320998   2.430742   0.301408   0.415376  \n",
      "    23     2.246791   2.420592   0.316901   0.425822  \n",
      "    24     2.293644   2.349373   0.326761   0.442606  \n",
      "    25     2.201538   2.316184   0.362676   0.470305  \n",
      "    26     2.166153   2.291447   0.352817   0.460094  \n",
      "    27     2.122408   2.366613   0.33169    0.442723  \n",
      "    28     2.071922   2.289296   0.346479   0.460211  \n",
      "    29     2.007761   2.223441   0.367606   0.480751  \n",
      "    30     2.074226   2.13038    0.409859   0.513498  \n",
      "    31     2.019593   2.190353   0.392254   0.499531  \n",
      "    32     1.995234   2.194614   0.382394   0.489906  \n",
      "    33     1.894331   2.189691   0.375352   0.487559  \n",
      "    34     1.971259   2.13149    0.395775   0.501995  \n",
      "    35     1.891739   2.086451   0.419718   0.521127  \n",
      "    36     1.841681   2.047329   0.428169   0.52946   \n",
      "    37     1.864351   2.051597   0.414085   0.523826  \n",
      "    38     1.807319   2.090358   0.402817   0.507864  \n",
      "    39     1.807078   2.055454   0.410563   0.517019  \n",
      "    40     1.830815   2.057553   0.426056   0.524883  \n",
      "    41     1.726285   2.006661   0.442254   0.539671  \n",
      "    42     1.779381   2.109697   0.414085   0.514085  \n",
      "    43     1.702771   2.072242   0.417606   0.521127  \n",
      "    44     1.784382   2.087397   0.421127   0.522535  \n",
      "    45     1.814571   2.026326   0.433803   0.536385  \n",
      "    46     1.700485   1.993503   0.443662   0.54284   \n",
      "    47     1.648531   2.034625   0.412676   0.524178  \n",
      "    48     1.739126   2.033083   0.421831   0.530634  \n",
      "    49     1.682655   2.000972   0.440845   0.54331   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.000971834424516, 0.440845070338585, 0.5433098591549295]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "learn.fit(lr, 1, cycle_len=50, use_clr_beta=(5, 25, 0.95, 0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_cycle('1d_rnn_3_layers_nh_128_16KHz', 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
