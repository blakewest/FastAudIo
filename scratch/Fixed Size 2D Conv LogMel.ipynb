{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Lambda\n",
    "\n",
    "from fastai.dataloader import DataLoader\n",
    "from fastai.dataset import get_cv_idxs, split_by_idx, ArraysIndexDataset, ModelData\n",
    "from fastai.metrics import accuracy\n",
    "from fastai.model import fit, predict\n",
    "\n",
    "from data_loading_utils import load_audio_files, read_file\n",
    "from preprocessing_utils import load_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data/')\n",
    "TRAIN_PATH = PATH/'audio_train'\n",
    "TEST_PATH = PATH/'audio_test'\n",
    "\n",
    "sample_rate = 44100\n",
    "n_segments = 220  # approx 2.5 seconds\n",
    "n_features = 80\n",
    "n_fft = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb2f7aa077b4c4ba0f9056475eb0be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing log_mel_spec features..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae81618ec0f42fa9ab4cbe46ad9827c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving data..\n",
      "Loaded features for 9473 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9473, 9473)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(PATH/'train.csv')\n",
    "\n",
    "labels = sorted(train.label.unique())\n",
    "label_idx = {label:i for i, label in enumerate(labels)}\n",
    "\n",
    "x = load_features(TRAIN_PATH, filenames=train.fname, feature_name='log_mel_spec', n_fft=n_fft, n_features=n_features)\n",
    "y = train.label.apply(lambda l: label_idx[l]).values\n",
    "len(x), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80, 1206), (9473,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from blake\n",
    "def get_trn_val_split(x, y, val_pct=0.15):\n",
    "    val_idxs = get_cv_idxs(len(x), val_pct=val_pct)\n",
    "    if isinstance(x, list):\n",
    "        return [([arr[i] for i in val_idxs], [arr[i] for i in range(len(arr)) if i not in val_idxs]) for arr in [x,y]]\n",
    "    else:\n",
    "        return split_by_idx(val_idxs, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8053, 8053, 1420, 1420)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((val_x, trn_x), (val_y, trn_y)) = get_trn_val_split(x, y, 0.15)\n",
    "\n",
    "len(trn_x), len(trn_y), len(val_x), len(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_subset2d(x, n):    \n",
    "    if x.shape[0] > n:\n",
    "        offset = np.random.randint(x.shape[0] - n)\n",
    "        return x[offset:offset+n]\n",
    "    elif x.shape[0] < n:\n",
    "        pad_total = n - x.shape[0]\n",
    "        pad_start = np.random.randint(pad_total)\n",
    "        pad_end = pad_total - pad_start\n",
    "        return np.pad(x, ((pad_start, pad_end), (0, 0)), mode='constant') # zeros\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomOffsetArraysIndexDataset(ArraysIndexDataset):\n",
    "    def __init__(self, x, y, n_segments, transform=None):\n",
    "        self.n_segments = n_segments\n",
    "        assert(len(x)==len(y))\n",
    "        super().__init__(x, y, transform)\n",
    "    \n",
    "    def get_x(self, i):\n",
    "        data = self.x[i].T\n",
    "        return random_subset2d(data, self.n_segments)\n",
    "    \n",
    "    def get_sz(self):\n",
    "        return self.n_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(n_in, n_out):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(n_in, n_out, kernel_size=(7, 7), padding=(3, 3)),\n",
    "        nn.BatchNorm2d(n_out),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Dropout(0.2)\n",
    "    )\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super().__init__()\n",
    "        self.lambd = lambd\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "class AudioCNN_MFCC(nn.Module):\n",
    "    def __init__(self, n_classes, n_segments, n_features):\n",
    "        super().__init__()\n",
    "\n",
    "        linear_input_ch = (n_features//16)*(n_segments//16) * 64\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            # Add in channel dimension\n",
    "            Lambda(lambda x: x.view(x.shape[0], 1, x.shape[1], x.shape[2])),\n",
    "            conv_block(1, 32),\n",
    "            conv_block(32, 32),\n",
    "            conv_block(32, 64),\n",
    "            conv_block(64, 64),\n",
    "            Lambda(lambda x: x.view(x.shape[0], -1)),\n",
    "            nn.Linear(linear_input_ch, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " # ArraysIndexDataset expects np arrays\n",
    "trn_y, val_y = np.array(trn_y), np.array(val_y)\n",
    "\n",
    "trn_ds = RandomOffsetArraysIndexDataset(trn_x, trn_y, n_segments)\n",
    "val_ds = RandomOffsetArraysIndexDataset(val_x, val_y, n_segments)\n",
    "trn_dl = DataLoader(trn_ds, shuffle=True, batch_size=16)\n",
    "val_dl = DataLoader(val_ds, shuffle=False, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 220, 80]), torch.Size([16]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, y1 = next(iter(trn_dl))\n",
    "x1.size(), y1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AudioCNN_MFCC(len(labels), n_segments, n_features).cuda()\n",
    "\n",
    "md = ModelData(PATH, trn_dl, val_dl)\n",
    "opt = optim.Adam(model.parameters())\n",
    "metrics = [accuracy]\n",
    "loss = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0596f3fa5f404643a1761e4821398348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy   \n",
      "    0      3.143239   3.188856   0.108451  \n",
      "    1      2.668115   2.476378   0.309155  \n",
      "    2      2.460484   2.666632   0.258451  \n",
      "    3      2.343896   2.324785   0.345775  \n",
      "    4      2.192679   2.150116   0.410563  \n",
      "    5      2.058338   1.976179   0.455634  \n",
      "    6      2.500651   2.442777   0.31831   \n",
      "    7      1.952349   1.905239   0.472535  \n",
      "    8      1.801051   1.823379   0.506338  \n",
      "    9      1.687673   1.863748   0.48169   \n",
      "    10     1.730925   1.926121   0.475352  \n",
      "    11     1.735418   1.680195   0.516901  \n",
      "    12     1.71097    1.681556   0.542958  \n",
      "    13     1.871553   1.854797   0.497183  \n",
      "    14     1.534922   1.677012   0.544366  \n",
      "    15     1.585034   1.698259   0.540141  \n",
      "    16     1.51044    1.639519   0.55493   \n",
      "    17     1.628853   2.084307   0.441549  \n",
      "    18     1.448779   1.664785   0.542958  \n",
      "    19     1.909282   2.418951   0.358451  \n",
      "    20     1.550798   1.640004   0.552817  \n",
      "    21     1.497575   1.599612   0.552817  \n",
      "    22     1.391902   1.589815   0.574648  \n",
      "    23     1.380554   1.623123   0.549296  \n",
      "    24     1.379802   1.567578   0.571831  \n",
      "    25     1.892737   2.073493   0.459155  \n",
      "    26     1.343081   1.543501   0.576056  \n",
      "    27     1.36333    1.51725    0.583803  \n",
      "    28     1.234225   1.537562   0.582394  \n",
      "    29     1.272973   1.574515   0.576761  \n",
      "    30     1.216825   1.505279   0.60493   \n",
      "    31     1.351321   1.625357   0.564789  \n",
      "    32     1.226734   1.472356   0.61338   \n",
      "    33     1.202203   1.553317   0.587324  \n",
      "    34     1.199101   1.462266   0.611268  \n",
      "    35     1.147319   1.475807   0.607746  \n",
      "    36     1.266406   1.467662   0.599296  \n",
      "    37     1.335269   1.586527   0.573944  \n",
      "    38     1.352984   1.496789   0.601408  \n",
      "    39     1.244748   1.381051   0.626761  \n",
      "    40     1.090768   1.471184   0.609859  \n",
      "    41     1.347171   1.490589   0.614789  \n",
      "    42     1.397195   1.62476    0.560563  \n",
      "    43     1.152146   1.490357   0.611268  \n",
      "    44     1.254879   1.513317   0.605634  \n",
      "    45     1.147721   1.401161   0.617606  \n",
      "    46     1.212012   1.528925   0.594366  \n",
      "    47     1.128147   1.393496   0.626056  \n",
      "    48     1.099868   1.633503   0.562676  \n",
      "    49     1.000212   1.459699   0.61338   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.459698949061649, 0.6133802816901408]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, md, n_epochs=50, crit=loss, opt=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'conv2d_1.w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e22735cfca4ad6ac61bfcc5a3889e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy   \n",
      "    0      0.857896   1.393606   0.628873  \n",
      "    1      0.884294   1.391728   0.65      \n",
      "    2      0.87205    1.389483   0.634507  \n",
      "    3      0.916998   1.350283   0.647183  \n",
      "    4      0.831819   1.420194   0.640845  \n",
      "    5      0.823298   1.346529   0.646479  \n",
      "    6      0.848156   1.378234   0.657042  \n",
      "    7      0.843087   1.36628    0.65493   \n",
      "    8      0.884747   1.35158    0.648592  \n",
      "    9      0.832478   1.353926   0.649296  \n",
      "    10     0.869389   1.369114   0.66338   \n",
      "    11     0.828256   1.378637   0.65493   \n",
      "    12     0.80068    1.393751   0.643662  \n",
      "    13     0.883887   1.393557   0.647887  \n",
      "    14     0.83453    1.368734   0.65493   \n",
      "    15     0.721828   1.361386   0.659859  \n",
      "    16     0.832753   1.370942   0.656338  \n",
      "    17     0.773136   1.381003   0.655634  \n",
      "    18     0.830521   1.355519   0.664085  \n",
      "    19     0.844764   1.401605   0.651408  \n",
      "    20     0.791343   1.365795   0.660563  \n",
      "    21     0.849177   1.386019   0.659859  \n",
      "    22     0.790081   1.399188   0.654225  \n",
      "    23     0.788813   1.377666   0.655634  \n",
      "    24     0.779143   1.381182   0.642958  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3811815940158467, 0.6429577464788733]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=0.0001)\n",
    "fit(model, md, n_epochs=25, crit=loss, opt=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data..\n",
      "Loaded features for 9400 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9400, 9400)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(PATH/'sample_submission.csv')\n",
    "test_x = load_features(TEST_PATH, filenames=test.fname, feature_name='log_mel_spec', n_fft=n_fft, n_features=n_features)\n",
    "test_y = np.zeros(len(test_x))\n",
    "len(test_x), len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the middle two seconds of the audio file to run the model on\n",
    "class AudioArraysIndexDataset(ArraysIndexDataset):\n",
    "    def __init__(self, x, y, n_segments, transform=None):\n",
    "        self.n_segments = n_segments\n",
    "        assert(len(x)==len(y))\n",
    "        super().__init__(x, y, transform)\n",
    "    \n",
    "    def get_x(self, i):\n",
    "        data = self.x[i].T\n",
    "        if data.shape[0] < self.n_segments:\n",
    "            data = np.pad(data, ((0, self.n_segments-data.shape[0]), (0, 0)), 'constant')\n",
    "        elif data.shape[0] > self.n_segments:\n",
    "            offset = (data.shape[0] - self.n_segments) // 2\n",
    "            data = data[offset:offset+self.n_segments]\n",
    "        return data\n",
    "    \n",
    "    def get_sz(self):\n",
    "        return self.n_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 220, 80])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = AudioArraysIndexDataset(test_x, test_y, n_segments)\n",
    "test_dl = DataLoader(test_ds, shuffle=False, batch_size=16)\n",
    "next(iter(test_dl))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9400, 41)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predict(model, test_dl)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the fizzbuzz starter kernel\n",
    "top_3 = np.array(labels)[np.argsort(-predictions, axis=1)[:, :3]]\n",
    "predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "test.label = predicted_labels\n",
    "test.to_csv('fixed_2d_conv_log_mel_spec.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
